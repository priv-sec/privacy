{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise_9_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiQhSVGZWMiU"
      },
      "source": [
        "## Exercise 9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clg9jgRxWI9v"
      },
      "source": [
        "### Imports and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnCFHZSli7DL"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.spatial.distance import cdist\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import random\n",
        "import sys\n",
        "\n",
        "from sklearn import datasets as dsets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from itertools import cycle, islice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HklmgORtWBPq"
      },
      "source": [
        "### DP-Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x6V-tj9Epla"
      },
      "source": [
        "# Function to compute the required sigma to achieve (<eps>, <delta>)-DP after <nb_iterations> iterations of the Gauss mechanism\n",
        "def compute_gaussian_sigma(eps, nb_iterations, delta=1e-5):\n",
        "\tdelta_term = 2 * (np.log(1.25)-np.log(delta))\n",
        "\treturn np.sqrt(nb_iterations * delta_term) / eps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zPHnPkfjt2L"
      },
      "source": [
        "# Clip the given value <val> at length <c>\n",
        "def clip_len(val, c):\n",
        "    ###### TODO (Part 1) ######\n",
        "    # Implement  the clipping #\n",
        "    ########## TODO ##########"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__Z7j_05jD4A"
      },
      "source": [
        "# DP-Clustering: Cluster the data points <x_given> into <k> clusters using <nb_iterations>. The initial centroids are <init_centroids>.\n",
        "# To achieve DP, to be protected values are bounded in length by clipping bound <c_bound> and perturbed with noise scaled by <noise_multiplier>.\n",
        "def dp_clustering(x_given, k, nb_iterations, eps, init_centroids, params):\n",
        "    # Set initial centroids/cluster centers\n",
        "    x = copy.deepcopy(x_given)\n",
        "    centroids = copy.deepcopy(init_centroids)\n",
        "\n",
        "    # Read given parameters\n",
        "    c_bound = params['c_bound']\n",
        "    noise_multiplier = compute_gaussian_sigma(eps, nb_iterations)\n",
        "    \n",
        "    # Calculate the distances between centroids and all data points\n",
        "    distances = cdist(x, centroids, 'euclidean')\n",
        "     \n",
        "    # Label data points by nearest centroid\n",
        "    labels = np.array([np.argmin(i) for i in distances])\n",
        "\n",
        "    # Create storage for average minimum distances between centroids and their assigned data points\n",
        "    avg_min_dist = []\n",
        "     \n",
        "    # For the defined number of iterations... \n",
        "    for _ in range(nb_iterations):\n",
        "        # For all clusters...\n",
        "        for idx in range(k):\n",
        "            # Skip clusters with no data points assigned\n",
        "            if len(x[labels==idx]) == 0:\n",
        "                print(\"Info: Empty cluster created.\")\n",
        "                continue\n",
        "            \n",
        "            ########## TODO (Part 1) ##########\n",
        "            # Implement the  centroid updates #\n",
        "            ############## TODO  ##############\n",
        "        \n",
        "        # Calculate average minimum distance between centroids and datapoints after current iteration\n",
        "        distances = cdist(x, centroids ,'euclidean')\n",
        "        min_dist = np.array([np.amin(i) for i in distances])\n",
        "        avg_min_dist.append(np.mean(min_dist))\n",
        "        labels = np.array([np.argmin(i) for i in distances])\n",
        "    \n",
        "    # Return final data points' cluster labels, final centroids and final average minimum distance\n",
        "    return labels, centroids, avg_min_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRRuaQstWDxg"
      },
      "source": [
        "### ExpQ-Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a-8vEgmpv-H"
      },
      "source": [
        "def _u(d, i, m):\n",
        "    return i + 1 - m if i < m else m - i\n",
        "\n",
        "\n",
        "def _b_size(b):\n",
        "    return np.abs( b[1] - b[0] )\n",
        "\n",
        "\n",
        "def _exp_mechanism(d, B, m, eps):\n",
        "\n",
        "    b_sizes = np.apply_along_axis( _b_size, 1, B )\n",
        "\n",
        "    utilities = np.fromiter( (_u(d, i, m) for i in np.indices((B.shape[0],))[0]), np.float64 ) \n",
        "    \n",
        "    weights = b_sizes * np.exp( eps * utilities )\n",
        "    weights /= weights.sum()\n",
        "    \n",
        "    return weights\n",
        "\n",
        "\n",
        "def _exp_q(d, eps):\n",
        "\n",
        "    m = np.floor( d.shape[0] / 2 ).astype(int)  # Desired quantile -> Median\n",
        "\n",
        "    B = np.array( [(x,y) for x,y in zip(d,d[1:])] )\n",
        "\n",
        "    weights = _exp_mechanism( d, B, m, eps )\n",
        "\n",
        "    indices = np.random.choice( np.indices((B.shape[0],))[0], p=weights, size=1 )\n",
        "    bins = B[indices]\n",
        "\n",
        "    unif_bin = lambda b: np.random.uniform(low=b[0], high=b[1])\n",
        "    values = np.apply_along_axis( unif_bin, 1, bins )\n",
        "\n",
        "    return values\n",
        "\n",
        "\n",
        "def get_median(data, eps):\n",
        "    \"\"\"\n",
        "    Note: <data> should be of the form [ [x1, y1, ...], [x2, y2, ...], ..., [xn, yn, ...] ]\n",
        "    \"\"\"\n",
        "\n",
        "    data = np.sort( data.T, axis=1 )\n",
        "\n",
        "    return np.array([ _exp_q(x, eps) for x in data ]).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBr9_1cepNHB"
      },
      "source": [
        "# ExpQ-Clustering: Use the median calculated using ExpQ to cluster the data points <x_given> into <k> clusters using <nb_iterations> iterations.\n",
        "# The initial centroids are <init_centroids>.\n",
        "def expq_clustering(x_given, k, nb_iterations, eps_target, init_centroids):\n",
        "\n",
        "    eps = eps_target/nb_iterations\n",
        "\n",
        "    # Set initial centroids/cluster centers\n",
        "    x = copy.deepcopy(x_given)\n",
        "    centroids = copy.deepcopy(init_centroids)\n",
        "\n",
        "    # Calculate the distances between centroids and all data points\n",
        "    distances = cdist(x, centroids, 'euclidean')\n",
        "     \n",
        "    # Label data points by nearest centroid\n",
        "    labels = np.array([np.argmin(i) for i in distances])\n",
        "\n",
        "    # Create storage for average minimum distances between centroids and their assigned data points\n",
        "    avg_min_dist = []\n",
        "     \n",
        "    # For the defined number of iterations... \n",
        "    for _ in range(nb_iterations):\n",
        "        # For all clusters...\n",
        "        for idx in range(k):\n",
        "            # Skip clusters with no data points assigned\n",
        "            if len(x[labels==idx]) == 0:\n",
        "                print(\"Info: Empty cluster created.\")\n",
        "                continue\n",
        "            \n",
        "            ########## TODO (Part 2) ##########\n",
        "            # Implement the  centroid updates #\n",
        "            ############## TODO  ##############\n",
        "        \n",
        "        # Calculate average minimum distance between centroids and datapoints after current iteration\n",
        "        distances = cdist(x, centroids ,'euclidean')\n",
        "        min_dist = np.array([np.amin(i) for i in distances])\n",
        "        avg_min_dist.append(np.mean(min_dist))\n",
        "        labels = np.array([np.argmin(i) for i in distances])\n",
        "    \n",
        "    # Return final data points' cluster labels, final centroids and final average minimum distance\n",
        "    return labels, centroids, avg_min_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_bMJzkFWYiw"
      },
      "source": [
        "### Gather and show results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKH7OI0XjLCF"
      },
      "source": [
        "# Use random.seed to get comparable results \n",
        "np.random.seed(42)\n",
        "\n",
        "# Create datasets\n",
        "# We use the iris dataset, split into two-dimensional subsets\n",
        "iris = dsets.load_iris()\n",
        "iris2d_1 = (iris.data[:,(0,1)], iris.target)\n",
        "iris2d_2 = (iris.data[:,(0,2)], iris.target)\n",
        "iris2d_3 = (iris.data[:,(1,2)], iris.target)\n",
        "iris2d_4 = (iris.data[:,(1,3)], iris.target)\n",
        "iris2d_5 = (iris.data[:,(2,3)], iris.target)\n",
        "\n",
        "# Prepare the result figure\n",
        "plt.figure(figsize=(25,25))\n",
        "#plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
        "plot_num = 1\n",
        "\n",
        "# Set default parameters\n",
        "default_params = {'n_clusters': 3,\n",
        "                  'iterations': 3,\n",
        "                  'eps': 1. }\n",
        "\n",
        "# Set datasets\n",
        "datasets = [\n",
        "    iris2d_1,\n",
        "    iris2d_2,\n",
        "    iris2d_3,\n",
        "    iris2d_4,\n",
        "    iris2d_5,\n",
        "    ]\n",
        "\n",
        "# Set clustering algorithms\n",
        "clustering_algorithms = (\n",
        "    ('DP-Clustering', dp_clustering, {'c_bound': .1}),\n",
        "    ###### TODO (Part 2) #######\n",
        "    # Also use ExpQ-Clustering #\n",
        "    # Simply uncomment this line: #\n",
        "    # ('ExpQ-Clustering', expq_clustering, {}),\n",
        ")\n",
        "\n",
        "# For all datasets...\n",
        "for i_dataset, dataset in enumerate(datasets):\n",
        "    params = default_params.copy()\n",
        "\n",
        "    # Prepare the current dataset\n",
        "    X, _ = dataset\n",
        "    X = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # From all data points: Choose fixed initial centroids to get comparable results for different algorithms\n",
        "    idx = np.random.choice(len(X), params[\"n_clusters\"], replace=False)\n",
        "    init_centroids = copy.deepcopy(X[idx, :])\n",
        "\n",
        "    # For all clustering algorithms...\n",
        "    for name, algorithm, algo_params in clustering_algorithms:\n",
        "        if algo_params:\n",
        "            y_pred, centroids, avg_distances = algorithm(X, params['n_clusters'], params['iterations'], params['eps'], init_centroids, algo_params)\n",
        "        else:\n",
        "            y_pred, centroids, avg_distances = algorithm(X, params['n_clusters'], params['iterations'], params['eps'], init_centroids)\n",
        "        \n",
        "        # Create new subplot\n",
        "        plt.subplot(len(datasets), len(clustering_algorithms)*2, plot_num)\n",
        "        if i_dataset == 0:\n",
        "            plt.title(f\"{name} (result)\", size=18)\n",
        "        \n",
        "        colors = np.array(list(islice(cycle(['#377eb8', '#ff7f00', '#4daf4a',\n",
        "                                             '#f781bf', '#a65628', '#984ea3',\n",
        "                                             '#999999', '#e41a1c', '#dede00']),\n",
        "                                      int(max(y_pred) + 1))))\n",
        "\n",
        "        # Plot the resulting clustering\n",
        "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n",
        "        plt.scatter(centroids[:,0], centroids[:,1], c=\"k\", marker=\"x\", label=\"centroid\")\n",
        "        plt.xticks(())\n",
        "        plt.yticks(())\n",
        "\n",
        "        plot_num += 1\n",
        "\n",
        "        plt.subplot(len(datasets), len(clustering_algorithms)*2, plot_num)\n",
        "        if i_dataset == 0:\n",
        "            plt.title(f\"{name} (performance)\", size=18)\n",
        "        plt.plot(avg_distances)\n",
        "        plt.xticks(range(params['iterations']))\n",
        "        plt.xlabel(\"iteration\")\n",
        "        plt.ylabel(\"avg_min_distance\")\n",
        "\n",
        "        plot_num += 1\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}